{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from fastrl.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fastrl\n",
    "\n",
    "> A Concise introduction to key ideas in RL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pip install fastrl`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following notes is targeted at people interested in Reinforcement Learning looking to get upto speed  without sacfricing on rigor.The necessary math is tackled but with computation first approach.Basic ability to write and understand simple programs and high school level math is all that is required.For readers looking to understand the latest developments in this field refer amazing [Spinning Up RL](https://spinningup.openai.com/en/latest/)\n",
    "\n",
    "\n",
    "### While reading the text reader should constantly be asking two questions :\n",
    "\n",
    "\n",
    "How can I implement this ? : \n",
    "\n",
    "`Brings practical computational problems into the picture and ensures actual understanding.`\n",
    "\n",
    "How can I be sure what I'm doing will work(If implemented correctly) ? : \n",
    "\n",
    "`As we will see RL needs lot of tricks and hacks to make it work. Whenever some notation and math help us in choosing sensible things,we should do it. This is where the rigor part of this notes comes into picture.This will be essential in removing the magic out of many **SOTA** algorithms in RL.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with humans learning from experiance.Say we are trying to learn to bicycle. We are driven by a goal to `stay balanced and pedal`.Along the way we fall.Now we need to start again.Somewhere in the gap between each iteration,we are learning and improving.Let's hash out the features of this process,if we are to model this :\n",
    "\n",
    "1. **Past actions influence future output**: There is no immediate feedback.Each micro-action(exerting more pressure on the pedal,..) along the way either leads to falling-off balance or keep moving.\n",
    "\n",
    "\n",
    " `Computational Problem` : How to assign credit to actions when they are not temporally connected ?\n",
    " \n",
    " \n",
    "\n",
    "2. **Outcomes might not be deterministic**: There are features of the environment(road,weather etc) that we do not fully understand that can effect outcome of the action.\n",
    "\n",
    "`Computational Problem`: How to make inference about the dynamics of a system when i\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
