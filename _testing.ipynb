{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### learning about the system with the interactions to the systems.\n",
    "-> rewards and punishments can be delayed in time from the action.(temporal disconnect.) <br>\n",
    "-> u might not get an evaluation from the input immediately.(state and reward is our imagination)<br>\n",
    "-> why not put vector reward.\n",
    "-> I do not have errors at every point/action.eg:writing an exam\n",
    "->Fundamental idea in rl\n",
    " * Intuition: The predictions i have at 't+1' are better than those at 't'(temporal difference learning)\n",
    " \n",
    "### create a tic-tac-toe in python and try to find the solution.\n",
    "\n",
    "->Say i'm at a particular board position and i managed to have some estimate of expected rewards..then i got to explore...say i'm at a board position with possible next step probabilities as (0.2,0.8,0.1) ..for fun i took the one with 0.2..then shall i update my vs.\n",
    "\n",
    "\n",
    "---------\n",
    "start with immediate reinforcement learning problem.\n",
    "Give a drug and test(at,rt,and )..outcome can be stochastic...(what is the optimal way to learn from a stocahstice environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Developing practical insight into probability. - motivating question.\n",
    "(how to know about a stochastic environment...given you can sample.?)\n",
    "\n",
    "-> coin flip,bayes rule,prior -> optimallearning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   8.,   91.,  534., 1712., 2926., 2731., 1516.,  423.,   53.,\n",
       "           6.]),\n",
       " array([-3.8676198 , -3.082831  , -2.298042  , -1.5132532 , -0.72846437,\n",
       "         0.05632448,  0.8411133 ,  1.6259022 ,  2.410691  ,  3.1954799 ,\n",
       "         3.9802687 ], dtype=float32),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0059), tensor(0.9857))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.mean(),a.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While reading the text reader should constantly be asking two questions:\n",
    "    1.How can i implement this.-> 'ensures actual understanding and also brings engineering problems into the picture.'\n",
    "    2.How can i be sure what i'm doing will work if implemented correctly. -> 'ensures rigor.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Immediate RL.\n",
    "\n",
    "A = [1,2,3...n]\n",
    "\n",
    "* Multi-arm Bandit problems:solution concepts\n",
    "-> Asymptotic convergence.\n",
    "-> Regret optimality\n",
    "-> PAC complexity.(probably approximately correct)\n",
    "\n",
    "1.value function based methods - q.\n",
    "a.'E' greedy approach\n",
    "b.softmax approach.(more intelligent approach.)\n",
    "\n",
    "questions: What is the effect of using constant 'alpha' in case of stationary problems.(u will keep swinging back n forth.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bandit based problems:\n",
    "Regret optimality.\n",
    "UCB(upper confidence bound):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(obs_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
    "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])\n",
    "\n",
    "\n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        step = EpisodeStep(observation=obs, action=action)\n",
    "        episode_steps.append(step)\n",
    "        if is_done:\n",
    "            e = Episode(reward=episode_reward, steps=episode_steps)\n",
    "            batch.append(e)\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "        \n",
    "        \n",
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        act_probs_v = sm(net(obs_v))\n",
    "        act_probs = act_probs_v.data.numpy()[0]\n",
    "        action = np.random.choice(len(act_probs), p=act_probs)\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        episode_reward += reward\n",
    "        step = EpisodeStep(observation=obs, action=action)\n",
    "        episode_steps.append(step)\n",
    "        if is_done:\n",
    "            e = Episode(reward=episode_reward, steps=episode_steps)\n",
    "            batch.append(e)\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs\n",
    "\n",
    "\n",
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for reward, steps in batch:\n",
    "        if reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step: step.observation, steps))\n",
    "        train_act.extend(map(lambda step: step.action, steps))\n",
    "\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vinay/anaconda3/envs/fastai2/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.667, reward_mean=16.6, rw_bound=18.0\n",
      "1: loss=0.679, reward_mean=25.4, rw_bound=32.0\n",
      "2: loss=0.698, reward_mean=25.6, rw_bound=27.5\n",
      "3: loss=0.668, reward_mean=23.2, rw_bound=26.0\n",
      "4: loss=0.661, reward_mean=23.7, rw_bound=26.5\n",
      "5: loss=0.663, reward_mean=28.8, rw_bound=35.0\n",
      "6: loss=0.651, reward_mean=39.9, rw_bound=38.5\n",
      "7: loss=0.650, reward_mean=36.7, rw_bound=41.0\n",
      "8: loss=0.623, reward_mean=37.5, rw_bound=48.5\n",
      "9: loss=0.623, reward_mean=52.4, rw_bound=58.5\n",
      "10: loss=0.623, reward_mean=56.4, rw_bound=59.5\n",
      "11: loss=0.606, reward_mean=58.3, rw_bound=67.0\n",
      "12: loss=0.598, reward_mean=46.6, rw_bound=52.5\n",
      "13: loss=0.589, reward_mean=50.6, rw_bound=58.5\n",
      "14: loss=0.596, reward_mean=63.3, rw_bound=73.5\n",
      "15: loss=0.593, reward_mean=68.1, rw_bound=70.0\n",
      "16: loss=0.567, reward_mean=65.3, rw_bound=71.5\n",
      "17: loss=0.570, reward_mean=73.6, rw_bound=86.5\n",
      "18: loss=0.569, reward_mean=69.8, rw_bound=77.5\n",
      "19: loss=0.553, reward_mean=81.2, rw_bound=89.5\n",
      "20: loss=0.534, reward_mean=74.4, rw_bound=86.0\n",
      "21: loss=0.548, reward_mean=86.4, rw_bound=95.5\n",
      "22: loss=0.545, reward_mean=98.3, rw_bound=116.0\n",
      "23: loss=0.532, reward_mean=90.2, rw_bound=92.5\n",
      "24: loss=0.531, reward_mean=109.6, rw_bound=107.5\n",
      "25: loss=0.531, reward_mean=112.4, rw_bound=123.5\n",
      "26: loss=0.556, reward_mean=132.6, rw_bound=148.5\n",
      "27: loss=0.538, reward_mean=165.4, rw_bound=200.0\n",
      "28: loss=0.540, reward_mean=179.9, rw_bound=200.0\n",
      "29: loss=0.536, reward_mean=177.6, rw_bound=200.0\n",
      "30: loss=0.548, reward_mean=181.6, rw_bound=200.0\n",
      "31: loss=0.533, reward_mean=188.0, rw_bound=200.0\n",
      "32: loss=0.533, reward_mean=172.6, rw_bound=200.0\n",
      "33: loss=0.536, reward_mean=190.4, rw_bound=200.0\n",
      "34: loss=0.535, reward_mean=176.8, rw_bound=200.0\n",
      "35: loss=0.541, reward_mean=169.8, rw_bound=200.0\n",
      "36: loss=0.550, reward_mean=169.8, rw_bound=200.0\n",
      "37: loss=0.526, reward_mean=187.4, rw_bound=200.0\n",
      "38: loss=0.523, reward_mean=188.8, rw_bound=200.0\n",
      "39: loss=0.525, reward_mean=183.2, rw_bound=200.0\n",
      "40: loss=0.523, reward_mean=191.8, rw_bound=200.0\n",
      "41: loss=0.516, reward_mean=194.2, rw_bound=200.0\n",
      "42: loss=0.523, reward_mean=195.8, rw_bound=200.0\n",
      "43: loss=0.516, reward_mean=194.6, rw_bound=200.0\n",
      "44: loss=0.526, reward_mean=198.7, rw_bound=200.0\n",
      "45: loss=0.514, reward_mean=200.0, rw_bound=200.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "    env = gym.make(\"CartPole-v0\")\n",
    "    # env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)\n",
    "    obs_size = env.observation_space.shape[0]\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "    objective = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "    \n",
    "    for iter_no, batch in enumerate(iterate_batches(\n",
    "            env, net, BATCH_SIZE)):\n",
    "        obs_v, acts_v, reward_b, reward_m = \\\n",
    "            filter_batch(batch, PERCENTILE)\n",
    "        optimizer.zero_grad()\n",
    "        action_scores_v = net(obs_v)\n",
    "        loss_v = objective(action_scores_v, acts_v)\n",
    "        loss_v.backward()\n",
    "        optimizer.step()\n",
    "        print(\"%d: loss=%.3f, reward_mean=%.1f, rw_bound=%.1f\" % (\n",
    "            iter_no, loss_v.item(), reward_m, reward_b))\n",
    "        if reward_m > 199:\n",
    "            print(\"Solved!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
